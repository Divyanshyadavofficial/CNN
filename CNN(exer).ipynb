{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " What are the advantages of a CNN over a fully connected DNN for image classi‐\n",
    "fication?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Spatial Hierarchy Preservation\n",
    "2. Parameter Efficiency\n",
    "3. Translation Invariance\n",
    "4. Better Feature Extraction\n",
    "5. Scalability to High-Resolution Images\n",
    "6. Improved Generalization and Accuracy\n",
    "7. Less Overfitting\n",
    "8. Widely Supported and Proven\n",
    "\n",
    "CNNs are designed specifically for image data. They are more efficient, accurate, and scalable than fully connected DNNs for image classification. CNNs' ability to capture spatial features makes them the preferred choice in most computer vision applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Consider a CNN composed of three convolutional layers, each with 3 × 3 kernels,\n",
    "a stride of 2, and \"same\" padding. The lowest layer outputs 100 feature maps, the\n",
    "middle one outputs 200, and the top one outputs 400. The input images are RGB\n",
    "images of 200 × 300 pixels.\n",
    "What is the total number of parameters in the CNN? If we are using 32-bit floats,\n",
    "at least how much RAM will this network require when making a prediction for a\n",
    "single instance? What about when training on a mini-batch of 50 images?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Given:\n",
    "\n",
    "Input: RGB image of size 200×300×3\n",
    "Three convolutional layers\n",
    "\n",
    "\n",
    "Kernel size = 3×3\n",
    "Stride = 2\n",
    "Padding = \"same\"\n",
    "Output feature maps:\n",
    "Layer 1 → 100 filters\n",
    "Layer 2 → 200 filters\n",
    "Layer 3 → 400 filters\n",
    "\n",
    "\n",
    "Data type: 32-bit floats (4 bytes per value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because stride = 2 and padding = \"same\", output size becomes:\n",
    "\n",
    "output_size = ceil(input_size / stride)\n",
    "\n",
    "\n",
    "Layer 1:\n",
    "Input: 200×300×3\n",
    "Output size:\n",
    "Height: ceil(200 / 2) = 100\n",
    "Width: ceil(300 / 2) = 150\n",
    "Channels: 100\n",
    "\n",
    "\n",
    "→ Output: 100×150×100\n",
    "\n",
    "Layer 2:\n",
    "\n",
    "\n",
    "Input: 100×150×100\n",
    "Output size:\n",
    "Height: ceil(100 / 2) = 50\n",
    "Width: ceil(150 / 2) = 75\n",
    "Channels: 200\n",
    "\n",
    "\n",
    "→ Output: 50×75×200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layer 3:\n",
    "\n",
    "\n",
    "Input: 50×75×200\n",
    "Output size:\n",
    "Height: ceil(50 / 2) = 25\n",
    "Width: ceil(75 / 2) = 38 (actually 37.5, so rounded up)\n",
    "Channels: 400\n",
    "\n",
    "\n",
    "→ Output: 25×38×400\n",
    "\n",
    "Number of Parameters\n",
    "\n",
    "Each convolutional filter has:\n",
    "\n",
    "Kernel: 3×3,\n",
    "Input channels = from previous layer,\n",
    "Output channels = number of filters\n",
    "Don’t forget biases (1 per filter).\n",
    "\n",
    "Layer 1:\n",
    "\n",
    "\n",
    "Filters: 100\n",
    "Each filter: 3×3×3 = 27\n",
    "Total: 100 × 27 + 100 = 2800 parameters\n",
    "\n",
    "\n",
    "Layer 2:\n",
    "\n",
    "\n",
    "Filters: 200\n",
    "Each filter: 3×3×100 = 900\n",
    "Total: 200 × 900 + 200 = 180,200 parameters\n",
    "\n",
    "Layer 3:\n",
    "\n",
    "\n",
    "Filters: 400\n",
    "Each filter: 3×3×200 = 1800\n",
    "Total: 400 × 1800 + 400 = 720,400 parameters\n",
    "\n",
    "= 2800 + 180,200 + 720,400\n",
    "= 903,400 parameters\n",
    "\n",
    "Memory Requirement for Inference\n",
    "\n",
    "➤ For a single instance, memory is required for:\n",
    "a. Parameters:\n",
    "\n",
    "Each parameter = 4 bytes (32-bit float)\n",
    "Total = 903,400 × 4 = 3.61 MB\n",
    "b. Activations (intermediate outputs)\n",
    "\n",
    "Layer outputs:\n",
    "\n",
    "Layer 1: 100×150×100 = 1,500,000 → 6 MB\n",
    "Layer 2: 50×75×200 = 750,000 → 3 MB\n",
    "Layer 3: 25×38×400 = 380,000 → 1.52 MB\n",
    "→ Total activations: ~10.52 MB\n",
    "\n",
    "= Parameters + Activations\n",
    "= 3.61 MB + 10.52 MB\n",
    "= ~14.13 \n",
    "\n",
    "= Activations (FWD + BWD) + Param grads + Parameters\n",
    "= ~1.05 GB + 3.61 MB + 3.61 MB\n",
    "≈ ~1.06 GB\n",
    "\n",
    "| Requirement                    | Value          |\n",
    "| ------------------------------ | -------------- |\n",
    "| Total parameters               | **903,400**    |\n",
    "| RAM for 1 prediction           | **\\~14.13 MB** |\n",
    "| RAM for batch of 50 (training) | **\\~1.06 GB**  |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.  If your GPU runs out of memory while training a CNN, what are five things you\n",
    "could try to solve the problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| # | Solution                 | Impact                          |\n",
    "| - | ------------------------ | ------------------------------- |\n",
    "| 1 | Reduce batch size        | 💥 Huge memory savings          |\n",
    "| 2 | Gradient checkpointing   | 🔁 Saves memory, slower compute |\n",
    "| 3 | Mixed precision training | 🧠 Memory + speed gain          |\n",
    "| 4 | Simplify model           | ⚒ Structural fix                |\n",
    "| 5 | Gradient accumulation    | 🧮 Simulate large batches       |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.  Why would you want to add a max pooling layer rather than a convolutional\n",
    "layer with the same stride?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 1. Purpose Difference: Downsampling vs. Feature Extraction\n",
    "\n",
    "\n",
    "Max Pooling Layer:\n",
    "\n",
    "\n",
    "Designed specifically for downsampling.\n",
    "It reduces spatial dimensions while preserving important features (like edges or textures).\n",
    "Applies a simple max operation — no learnable parameters.\n",
    "\n",
    "\n",
    "Convolutional Layer with stride > 1:\n",
    "Performs feature extraction and downsampling simultaneously.\n",
    "Has trainable filters, increasing the model complexity and risk of overfitting.\n",
    "\n",
    "2. No Extra Parameters in Max Pooling\n",
    "\n",
    "\n",
    "Pooling layers don’t have weights → no additional memory or training time.\n",
    "Convolution layers have many parameters, especially with many filters\n",
    "\n",
    "3. Better Control Over Translation Invariance\n",
    "\n",
    "\n",
    "Max pooling introduces local translation invariance.\n",
    "If a feature shifts slightly, pooling still captures it.\n",
    "Strided convolution may miss small shifts unless filters are trained carefully.\n",
    "\n",
    "4. Regularization Effect\n",
    "\n",
    "\n",
    "Pooling reduces overfitting by reducing spatial size and making the network focus on the most prominent features.\n",
    "Strided convolutions may extract more details but can also memorize noise if not regularized properly.\n",
    "\n",
    "\n",
    "5. Computational Efficiency\n",
    "\n",
    "\n",
    "Max pooling is a simple operation (just max).\n",
    "Convolution with stride is more computationally expensive (involves multiplications and additions with weights).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. When would you want to add a local response normalization layer?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is LRN?\n",
    "\n",
    "\n",
    "Local Response Normalization (LRN) is a type of normalization layer that encourages competition between adjacent neurons (usually across channels).\n",
    "It was popularized by AlexNet (2012).\n",
    "\n",
    "When to Use It\n",
    "You might want to add an LRN layer in the following cases:\n",
    "\n",
    "1. To Enhance Generalization and Reduce Overfitting (in early CNNs)\n",
    "LRN acts as a regularizer, similar to dropout or batch normalization.\n",
    "It helps highlight high-activation neurons while suppressing weaker ones.\n",
    "Useful in older networks without batch normalization.\n",
    "\n",
    "\n",
    "2. To Promote Competition Between Feature Maps\n",
    "LRN applies normalization across nearby channels (depth-wise).\n",
    "Neurons that strongly activate \"suppress\" the activation of their neighbors.\n",
    "Encourages distinct, strong features to stand out.\n",
    "\n",
    "3. When Reproducing or Extending Legacy Models (e.g., AlexNet)\n",
    "If you're re-implementing or modifying older architectures, LRN is often used after ReLU activation.\n",
    "Helps maintain consistency with earlier benchmarks.\n",
    "\n",
    "4. On Shallow Networks or Small Datasets\n",
    "When using shallow CNNs or training on limited data, LRN can add regularization and boost robustness.\n",
    "Can slightly improve generalization when batch normalization is not used.\n",
    "\n",
    "When Not to Use It:\n",
    "\n",
    "\n",
    "Modern networks (like ResNet, VGG, MobileNet) don’t use LRN.\n",
    "Batch Normalization, Layer Norm, or Group Norm have largely replaced LRN due to:\n",
    "Better performance\n",
    "More stable gradients\n",
    "Faster convergence\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Can you name the main innovations in AlexNet, compared to LeNet-5? What\n",
    "about the main innovations in GoogLeNet, ResNet, SENet, and Xception?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. LeNet-5 (1998) — The Foundation\n",
    "\n",
    "One of the first CNNs, designed by Yann LeCun for digit recognition (MNIST).\n",
    "Architecture:\n",
    "2 convolutional layers + subsampling (pooling) + fully connected layers.\n",
    "Limitations:\n",
    "Shallow network.\n",
    "Couldn’t handle large images or datasets like ImageNet.\n",
    "Trained on CPU.\n",
    "\n",
    "\n",
    " 2. AlexNet (2012) — The Breakthrough\n",
    "\n",
    "Designed by Krizhevsky, Sutskever, Hinton — winner of ImageNet 2012.\n",
    "Innovations Compared to LeNet-5:\n",
    "Much deeper architecture (8 layers vs. 5 in LeNet).\n",
    "Used ReLU activation (faster convergence than sigmoid/tanh).\n",
    "Introduced Dropout to prevent overfitting.\n",
    "Used GPU acceleration for training.\n",
    "Employed Data Augmentation (cropping, flipping).\n",
    "Used Local Response Normalization (LRN).\n",
    "Used overlapping max pooling.\n",
    "\n",
    "\n",
    " 3. GoogLeNet (Inception-v1, 2014) — The Depth Revolution\n",
    "\n",
    "Developed by Szegedy et al., winner of ImageNet 2014.\n",
    "Key Innovations:\n",
    "Introduced the Inception module:\n",
    "Combines 1×1, 3×3, and 5×5 convolutions in parallel.\n",
    "Allows the network to learn multi-scale features.\n",
    "1×1 convolutions used for:\n",
    "Dimensionality reduction\n",
    "Reduced computation cost.\n",
    "Removed fully connected layers → replaced with Global Average Pooling.\n",
    "Much deeper (~22 layers) but still computationally efficient.\n",
    "\n",
    "\n",
    "4. ResNet (2015) — The Deep Learning Enabler\n",
    "\n",
    "Introduced by Kaiming He et al., winner of ImageNet 2015.\n",
    "Key Innovation:\n",
    "Introduced Residual Connections (Skip Connections):\n",
    "Solves the vanishing gradient problem.\n",
    "Enables training of very deep networks (up to 1000+ layers).\n",
    "Residual block:\n",
    "Output = F(x) + x \n",
    "Allows the network to learn identity mappings easily.\n",
    "\n",
    "\n",
    "5. SENet (Squeeze-and-Excitation Network, 2017) — Channel-Wise Attention\n",
    "\n",
    "Winner of ImageNet 2017.\n",
    "Key Innovations:\n",
    "Introduced Squeeze-and-Excitation (SE) blocks:\n",
    "Learn channel-wise attention weights.\n",
    "\"Squeeze\": Global average pooling to summarize feature maps.\n",
    "\"Excite\": Fully connected layers to learn importance of each channel.\n",
    "Improves accuracy by recalibrating feature maps channel-wise.\n",
    "Can be added to any CNN (ResNet, Inception, etc.).\n",
    "\n",
    "\n",
    "6. Xception (2017) — Extreme Inception + Depthwise Separable Convolutions\n",
    "\n",
    "Proposed by François Chollet (creator of Keras).\n",
    "Key Innovations:\n",
    "Replaces Inception modules with Depthwise Separable Convolutions:\n",
    "Depthwise convolution → applies one filter per input channel.\n",
    "Pointwise (1×1) convolution → combines outputs.\n",
    "Significantly reduces computation while maintaining performance.\n",
    "Fully convolutional, no Inception blocks.\n",
    "Inspired by Inception but with greater modularity and efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. What is a fully convolutional network? How can you convert a dense layer into a\n",
    "convolutional layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Fully Convolutional Network (FCN) is a type of neural network that consists only of convolutional layers (and possibly pooling and activation layers) — no fully connected (dense) layers.\n",
    "\n",
    "Key Characteristics:\n",
    "\n",
    "\n",
    "Accepts inputs of any size (not fixed like in traditional CNNs).\n",
    "Outputs spatial predictions, making it ideal for tasks like:\n",
    "Semantic segmentation\n",
    "Heatmap generation\n",
    "Object localization\n",
    "Example Use Case:\n",
    "\n",
    "\n",
    "In semantic segmentation, an FCN takes an image and outputs a pixel-wise classification map, labeling each pixel with a class.\n",
    "\n",
    "How to Convert a Dense Layer into a Convolutional Layer\n",
    "\n",
    "Converting a dense (fully connected) layer into a convolutional layer allows a CNN to become fully convolutional — enabling flexible input sizes and spatial output.\n",
    "\n",
    "Conceptual Mapping:\n",
    "Dense Layer\tEquivalent Conv Layer\n",
    "Input: Flattened vector\tInput: 2D feature map\n",
    "Dense: units = N\tConv2D: filters = N, kernel = H×W\n",
    "Output: 1D (N values)\tOutput: 1×1×N (if kernel covers full input)\n",
    "Key Idea:\n",
    "A dense layer is a 1×1 convolution applied over a flattened feature map.\n",
    "\n",
    "Conversion Steps:\n",
    "Let’s say you have a dense layer that operates on a flattened feature map:\n",
    "\n",
    "Original input: feature map of shape H×W×C\n",
    "Flattened: vector of size H×W×C = N\n",
    "Dense layer: e.g., 512 units → matrix multiplication: W × N\n",
    "To convert:\n",
    "\n",
    "Replace the dense layer with a convolutional layer:\n",
    "Use Conv2D with:\n",
    "Number of filters = number of units in Dense layer\n",
    "Kernel size = H×W (if you want full coverage)\n",
    "Or kernel size = 1×1 if spatial position should be preserved\n",
    "Adjust strides and padding appropriately.\n",
    "Example:\n",
    "Let’s say:\n",
    "\n",
    "You have a dense layer with 512 units.\n",
    "Input before flattening was 7×7×256.\n",
    "Replace:\n",
    "\n",
    "Dense(512) → Conv2D(512 filters, kernel size = 7×7)\n",
    "This will produce the same result, but allows the network to work with larger inputs, maintaining spatial structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. What is the main technical difficulty of semantic segmentation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Maintaining Spatial Accuracy\n",
    "Semantic segmentation requires predicting a class for every pixel.\n",
    "As CNNs go deeper, spatial resolution is reduced due to pooling/striding (e.g., from 256×256 to 8×8).\n",
    "Recovering precise object boundaries from low-resolution features is difficult.\n",
    "🔧 Solution attempts: Skip connections (FCN), dilated convolutions, encoder–decoder architectures (e.g., U-Net, DeepLab).\n",
    "\n",
    "\n",
    "2. Multi-Scale Object Recognition\n",
    "Objects in images appear at various sizes and shapes.\n",
    "A single receptive field may not capture both small and large objects accurately.\n",
    "🔧 Solution attempts: Pyramid pooling (PSPNet), Atrous Spatial Pyramid Pooling (ASPP in DeepLab), multi-scale feature fusion.\n",
    "\n",
    "\n",
    "3. Class Imbalance\n",
    "Background pixels often dominate the image (e.g., sky, road), while some classes (like a person or traffic light) may be very small.\n",
    "This can lead to bias in the loss function, where the model ignores rare classes.\n",
    "🔧 Solution attempts: Weighted loss functions, focal loss, over-sampling rare classes.\n",
    "\n",
    "\n",
    "4. Ambiguous Boundaries\n",
    "Adjacent classes often have unclear or fuzzy borders, like “cat” vs. “sofa”.\n",
    "Slight annotation noise or motion blur makes classification harder at edges.\n",
    "🔧 Solution attempts: Conditional Random Fields (CRF) post-processing, edge-aware networks.\n",
    "5. High Computational Cost\n",
    "Pixel-wise predictions require large memory and compute, especially for high-resolution images.\n",
    "\n",
    "\n",
    "Training and inference can be slow.\n",
    "🔧 Solution attempts: Model compression, efficient backbones (e.g., MobileNet, ENet)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Build your own CNN from scratch and try to achieve the highest possible accu‐\n",
    "racy on MNIST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers,models\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train,y_train),(x_test,y_test) = mnist.load_data()\n",
    "x_train = x_train.astype('float32')/255.0\n",
    "x_test = x_test.astype('float32')/255.0\n",
    "x_train = x_train[...,tf.newaxis]\n",
    "x_test = x_test[...,tf.newaxis]\n",
    "\n",
    "y_train = to_categorical(y_train,10)\n",
    "y_test = to_categorical(y_test,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential([\n",
    "    layers.Conv2D(32,(3,3),activation='relu',input_shape=(28,28,1)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Conv2D(64,(3,3),activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D(pool_size=(2,2)),\n",
    "    layers.Dropout(0.25),\n",
    "\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Dropout(0.25),\n",
    "\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "422/422 - 42s - 98ms/step - accuracy: 0.9434 - loss: 0.1889 - val_accuracy: 0.4052 - val_loss: 2.3912\n",
      "Epoch 2/15\n",
      "422/422 - 41s - 96ms/step - accuracy: 0.9802 - loss: 0.0663 - val_accuracy: 0.9880 - val_loss: 0.0400\n",
      "Epoch 3/15\n",
      "422/422 - 41s - 98ms/step - accuracy: 0.9838 - loss: 0.0529 - val_accuracy: 0.9888 - val_loss: 0.0402\n",
      "Epoch 4/15\n",
      "422/422 - 42s - 99ms/step - accuracy: 0.9861 - loss: 0.0439 - val_accuracy: 0.9907 - val_loss: 0.0306\n",
      "Epoch 5/15\n",
      "422/422 - 42s - 101ms/step - accuracy: 0.9891 - loss: 0.0358 - val_accuracy: 0.9915 - val_loss: 0.0277\n",
      "Epoch 6/15\n",
      "422/422 - 46s - 110ms/step - accuracy: 0.9900 - loss: 0.0323 - val_accuracy: 0.9925 - val_loss: 0.0246\n",
      "Epoch 7/15\n",
      "422/422 - 48s - 113ms/step - accuracy: 0.9907 - loss: 0.0292 - val_accuracy: 0.9930 - val_loss: 0.0255\n",
      "Epoch 8/15\n",
      "422/422 - 50s - 119ms/step - accuracy: 0.9915 - loss: 0.0270 - val_accuracy: 0.9933 - val_loss: 0.0222\n",
      "Epoch 9/15\n",
      "422/422 - 47s - 111ms/step - accuracy: 0.9926 - loss: 0.0243 - val_accuracy: 0.9933 - val_loss: 0.0249\n",
      "Epoch 10/15\n",
      "422/422 - 46s - 110ms/step - accuracy: 0.9926 - loss: 0.0233 - val_accuracy: 0.9920 - val_loss: 0.0289\n",
      "Epoch 11/15\n",
      "422/422 - 47s - 112ms/step - accuracy: 0.9930 - loss: 0.0218 - val_accuracy: 0.9940 - val_loss: 0.0223\n",
      "Epoch 12/15\n",
      "422/422 - 51s - 121ms/step - accuracy: 0.9946 - loss: 0.0175 - val_accuracy: 0.9943 - val_loss: 0.0228\n",
      "Epoch 13/15\n",
      "422/422 - 50s - 119ms/step - accuracy: 0.9938 - loss: 0.0189 - val_accuracy: 0.9940 - val_loss: 0.0206\n",
      "Epoch 14/15\n",
      "422/422 - 51s - 122ms/step - accuracy: 0.9947 - loss: 0.0162 - val_accuracy: 0.9920 - val_loss: 0.0257\n",
      "Epoch 15/15\n",
      "422/422 - 52s - 122ms/step - accuracy: 0.9944 - loss: 0.0163 - val_accuracy: 0.9940 - val_loss: 0.0250\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    x_train,y_train,\n",
    "    epochs=15,\n",
    "    batch_size=128,\n",
    "    validation_split = 0.1,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9935\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"Test accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Use transfer learning for large image classification, going through these steps:\n",
    "a. Create a training set containing at least 100 images per class. For example, you\n",
    "could classify your own pictures based on the location (beach, mountain, city,\n",
    "etc.), or alternatively you can use an existing dataset (e.g., from TensorFlow\n",
    "Datasets).\n",
    "b. Split it into a training set, a validation set, and a test set.\n",
    "c. Build the input pipeline, including the appropriate preprocessing operations,\n",
    "and optionally add data augmentation.\n",
    "d. Fine-tune a pretrained model on this datase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "(raw_train,raw_val,raw_test),metadata = tfds.load(\n",
    "    'tf_flowers',\n",
    "    split=['train[:80%]','train[80%:90%]','train[90%:]'],\n",
    "    with_info=True,\n",
    "    as_supervised=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "IMG_SIZE  = 224\n",
    "BATCH_SIZE = 32\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.Rescaling(1./255),\n",
    "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "    tf.keras.layers.RandomRotation(0.1),\n",
    "    tf.keras.layers.RandomZoom(0.1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image,label):\n",
    "    image = tf.image.resize(image,(IMG_SIZE,IMG_SIZE))\n",
    "    image = tf.cast(image,tf.float32)/255.0\n",
    "    return image,label\n",
    "train_ds = raw_train.map(preprocess_image).cache().shuffle(1000).batch(BATCH_SIZE).prefetch(1)\n",
    "val_ds = raw_val.map(preprocess_image).batch(BATCH_SIZE).prefetch(1)\n",
    "test_ds = raw_test.map(preprocess_image).batch(BATCH_SIZE).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
      "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "base_model = tf.keras.applications.MobileNetV2(\n",
    "    input_shape=(IMG_SIZE,IMG_SIZE,3),\n",
    "    include_top = False,\n",
    "    weights='imagenet'\n",
    ")\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers,models\n",
    "model = models.Sequential([\n",
    "    data_augmentation,\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(128,activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(5,activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 748ms/step - accuracy: 0.2162 - loss: 1.7261 - val_accuracy: 0.2425 - val_loss: 1.5951\n",
      "Epoch 2/5\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 771ms/step - accuracy: 0.2423 - loss: 1.6078 - val_accuracy: 0.2425 - val_loss: 1.6041\n",
      "Epoch 3/5\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 732ms/step - accuracy: 0.2457 - loss: 1.6053 - val_accuracy: 0.2425 - val_loss: 1.6008\n",
      "Epoch 4/5\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 803ms/step - accuracy: 0.2458 - loss: 1.6036 - val_accuracy: 0.2425 - val_loss: 1.5984\n",
      "Epoch 5/5\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 680ms/step - accuracy: 0.2547 - loss: 1.5999 - val_accuracy: 0.2425 - val_loss: 1.5968\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "history = model.fit(train_ds,validation_data=val_ds,epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 1s/step - accuracy: 0.2862 - loss: 1.5880 - val_accuracy: 0.2425 - val_loss: 1.5993\n",
      "Epoch 2/5\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 1s/step - accuracy: 0.3375 - loss: 1.5313 - val_accuracy: 0.2425 - val_loss: 1.6000\n",
      "Epoch 3/5\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m425s\u001b[0m 5s/step - accuracy: 0.3806 - loss: 1.4804 - val_accuracy: 0.2425 - val_loss: 1.6020\n",
      "Epoch 4/5\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 997ms/step - accuracy: 0.3673 - loss: 1.4861 - val_accuracy: 0.2425 - val_loss: 1.5997\n",
      "Epoch 5/5\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 1s/step - accuracy: 0.4017 - loss: 1.4519 - val_accuracy: 0.2425 - val_loss: 1.5929\n"
     ]
    }
   ],
   "source": [
    "base_model.trainable=True\n",
    "fine_tune_at = 100\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "    layer.trainable =False\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-5),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "history_fine = model.fit(train_ds,validation_data=val_ds,epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 265ms/step - accuracy: 0.2069 - loss: 1.6233\n",
      "Test accuracy: 0.19\n"
     ]
    }
   ],
   "source": [
    "test_loss,test_accuracy = model.evaluate(test_ds)\n",
    "print(f\"Test accuracy: {test_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
